# Projet d'Analyse des Donn√©es de Logs avec Hadoop et Flume

Ce projet impl√©mente un pipeline complet pour collecter, stocker et analyser des donn√©es de logs √† l'aide de technologies Big Data comme Hadoop, Flume et Hive. Il d√©montre comment transformer des donn√©es non structur√©es (logs) en informations exploitables.

## Architecture du Projet
[Log Generator] ‚Üí [Flume] ‚Üí [HDFS] ‚Üí [MapReduce] ‚Üí [Hive] ‚Üí üìä [Excel Results]

Le pipeline de donn√©es comprend les √©tapes suivantes :
1. **G√©n√©ration de logs** - Simulation de logs d'une application web
2. **Collecte de logs** - Utilisation de Flume pour capturer et transf√©rer les logs
3. **Stockage** - Utilisation de HDFS pour stocker les logs bruts
4. **Traitement** - Utilisation de MapReduce pour analyser les logs
5. **Analyse** - Utilisation de Hive pour permettre des requ√™tes SQL sur les donn√©es

## Pr√©requis

- Une machine virtuelle Ubuntu (18.04 ou plus r√©cente)
- Java 8 ou sup√©rieur
- Minimum 4 Go de RAM et 20 Go d'espace disque

## Installation

1. Clonez ce d√©p√¥t ou t√©l√©chargez et extrayez les fichiers dans votre VM Ubuntu
2. Rendez tous les scripts ex√©cutables :
   ```bash
   chmod +x *.sh
   ```
3. Ex√©cutez le script d'installation pour installer tous les composants n√©cessaires :
   ```bash
   ./installation.sh
   ```

Ce script installera Hadoop, Flume et Hive avec les configurations appropri√©es.

## Configuration de l'environnement

1. Configurez Hadoop en mode pseudo-distribu√© :
   ```bash
   ./hadoop-config.sh
   ```

2. Assurez-vous que tous les fichiers de configuration sont correctement configur√©s :
   - `weblog-flume.conf` : Configuration de l'agent Flume
   - Les fichiers XML de configuration Hadoop dans `$HADOOP_HOME/etc/hadoop/`

## D√©marrage du Pipeline

Vous pouvez d√©marrer chaque composant individuellement ou ex√©cuter le pipeline complet :

### Option 1 : Pipeline complet automatis√©

Ex√©cutez le script principal qui orchestre tout le pipeline :
```bash
./start-log-analysis.sh
```

Ensuite, choisissez l'option 8 dans le menu pour ex√©cuter le pipeline complet.

### Option 2 : D√©marrage manuel √©tape par √©tape

1. D√©marrez Hadoop :
   ```bash
   ./hadoop-start-stop.sh start
   ```

2. Lancez le g√©n√©rateur de logs simul√©s :
   ```bash
   python3 generate_logs.py &
   ```

3. D√©marrez l'agent Flume pour collecter les logs :
   ```bash
   ./start-flume.sh
   ```

4. Compilez le programme MapReduce :
   ```bash
   ./compile-mapreduce.sh
   ```

5. Ex√©cutez l'analyse MapReduce une fois que suffisamment de logs ont √©t√© collect√©s :
   ```bash
   ./run-mapreduce.sh
   ```

6. Configurez les tables Hive :
   ```bash
   ./setup-hive.sh
   ```

7. Ex√©cutez les requ√™tes d'analyse Hive :
   ```bash
   ./run-hive-analysis.sh
   ```

## Exploration des Donn√©es

Apr√®s avoir ex√©cut√© le pipeline, vous pouvez explorer les donn√©es de diff√©rentes mani√®res :

### Exploration directe des logs bruts dans HDFS
```bash
hadoop fs -ls /user/$USER/logs/raw/
hadoop fs -cat /user/$USER/logs/raw/YYYY/MM/DD/weblogs*.log | head -n 10
```

### Visualisation des r√©sultats MapReduce
```bash
hadoop fs -ls /user/$USER/logs/processed/
hadoop fs -cat /user/$USER/logs/processed/*/part-r-00000 | head -n 20
```

### Exploration avec Hive
1. Lancez la CLI Hive :
   ```bash
   $HIVE_HOME/bin/hive
   ```

2. Ex√©cutez des requ√™tes Hive :
   ```sql
   USE log_analysis;
   SHOW TABLES;
   SELECT * FROM page_trends LIMIT 10;
   ```

3. Vous pouvez √©galement ex√©cuter directement les requ√™tes du fichier `analyze-logs.hql` :
   ```bash
   $HIVE_HOME/bin/hive -f analyze-logs.hql
   ```

## Arr√™t du Pipeline

Pour arr√™ter tous les services en cours d'ex√©cution :
```bash
./stop-pipeline.sh
```

## D√©pannage

- **Probl√®me** : "Connection refused" lors de la connexion √† HDFS
  **Solution** : V√©rifiez que le NameNode est en cours d'ex√©cution avec `jps` et red√©marrez HDFS si n√©cessaire

- **Probl√®me** : Flume ne collecte pas les logs
  **Solution** : V√©rifiez que le r√©pertoire `/tmp/weblogs/` existe et que le fichier `access.log` y est pr√©sent

- **Probl√®me** : L'analyse MapReduce √©choue
  **Solution** : V√©rifiez que le chemin d'entr√©e dans HDFS existe et contient des donn√©es avec `hadoop fs -ls`

## Structure des Fichiers

```
.
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îú‚îÄ‚îÄ installation.sh            # Script d'installation des composants
‚îú‚îÄ‚îÄ hadoop-config.sh           # Configuration de Hadoop
‚îú‚îÄ‚îÄ hadoop-start-stop.sh       # Gestion des services Hadoop
‚îú‚îÄ‚îÄ generate_logs.py           # G√©n√©rateur de logs simul√©s
‚îú‚îÄ‚îÄ weblog-flume.conf          # Configuration de Flume
‚îú‚îÄ‚îÄ start-flume.sh             # D√©marrage de l'agent Flume
‚îú‚îÄ‚îÄ LogAnalyzer.java           # Programme MapReduce
‚îú‚îÄ‚îÄ compile-mapreduce.sh       # Compilation du programme MapReduce
‚îú‚îÄ‚îÄ run-mapreduce.sh           # Ex√©cution du job MapReduce
‚îú‚îÄ‚îÄ setup-hive.sh              # Configuration des tables Hive
‚îú‚îÄ‚îÄ analyze-logs.hql           # Requ√™tes d'analyse Hive
‚îú‚îÄ‚îÄ run-hive-analysis.sh       # Ex√©cution des analyses Hive
‚îú‚îÄ‚îÄ start-log-analysis.sh      # Script principal d'orchestration
‚îî‚îÄ‚îÄ stop-pipeline.sh           # Arr√™t de tous les services
```

## Technologies Utilis√©es

- **Hadoop** : Framework de traitement distribu√©
- **HDFS** : Syst√®me de fichiers distribu√© pour le stockage des logs
- **MapReduce** : Mod√®le de programmation pour le traitement des logs
- **Flume** : Service de collecte et d'agr√©gation de logs
- **Hive** : Entrep√¥t de donn√©es pour l'interrogation SQL

## Pour Aller Plus Loin

- Ajoutez d'autres sources de logs (serveurs multiples, applications diff√©rentes)
- Int√©grez Spark pour des analyses plus rapides
- Ajoutez un tableau de bord de visualisation avec des outils comme Superset
- Impl√©mentez la d√©tection d'anomalies en temps r√©el

# English Short Description :
##  Key Features
 - Log Generation: Python-based simulator for realistic web logs.
 
 - Real-time Ingestion: Apache Flume agents to collect and transfer logs to HDFS.

 - Distributed Storage: Hadoop HDFS for fault-tolerant, scalable storage.

 - Batch Processing: MapReduce jobs to parse, filter, and aggregate logs.

 - SQL-like Analysis: Apache Hive for querying log data with familiar syntax.

## Tech Stack
 - Hadoop (HDFS + MapReduce)

 - Apache Flume (Data ingestion)

 - Apache Hive (Data warehousing)

 - Python (Log simulation)

## Sample Use Cases
 - Track page visit trends and user behavior.

 - Detect 404 errors or suspicious traffic patterns.

 - Generate daily/weekly reports (e.g., top visited pages).

## Quick Start
* Prerequisites: Ubuntu VM, Java 8+, 4GB RAM.

## Explore Results
 - Raw Logs: hadoop fs -cat /user/logs/raw/*

 - Processed Output: hadoop fs -cat /user/logs/processed/*/part-r-00000

 - Hive Queries: Predefined in analyze-logs.hql (e.g., session analysis).

## Future Enhancements
 - Integrate Spark for real-time analytics.

 - Add Superset/Grafana dashboards.

 - Extend to multi-node clusters.

# Why This Project?
‚úî Hands-on learning for Big Data tools (Hadoop/Flume/Hive).
‚úî Production-ready pipeline with automation scripts.
‚úî Customizable for other log formats (e.g., IoT, APIs).
